{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import pickle\n",
    "import jieba\n",
    "import random\n",
    "#import re\n",
    "import pandas\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root=\"./CDC.csv\", seq_len = 40) -> None:\n",
    "        super(TextDataset, self).__init__()\n",
    "        self.frame = pandas.read_csv(root)\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab = build_vocab_from_iterator(self.get_vocab(), specials=[\"<start>\", \"'\"])\n",
    "        self.stoi = self.vocab.get_stoi()\n",
    "        self.itos = self.vocab.get_itos()\n",
    "        self.inputs = self.get_inputs()\n",
    "        pickle.dump(self.stoi, open(\"stoi.bin\", \"wb\"))\n",
    "        pickle.dump(self.itos, open(\"itos.bin\", \"wb\"))\n",
    "\n",
    "    def get_inputs(self):\n",
    "        #data = self.frame['text'].apply(lambda text : re.sub(r\"[^\\w\\s.]\", \" \", text)).to_list()\n",
    "        data = self.frame['Question'].to_list()\n",
    "        inputs = []\n",
    "        for text in data:\n",
    "            text_data = [self.stoi[chars] for chars in jieba.lcut(str(text).strip())]\n",
    "            text_data.insert(0, self.stoi[\"<start>\"])\n",
    "            while len(text_data) < self.seq_len:\n",
    "                text_data.append(self.stoi[\"'\"])\n",
    "            inputs.append(text_data[:self.seq_len])\n",
    "        inputs = numpy.array(inputs, dtype=numpy.int32)\n",
    "        inputs = torch.from_numpy(inputs).long()\n",
    "        return inputs\n",
    "    \n",
    "    def get_vocab(self):\n",
    "        #data = self.frame['text'].apply(lambda text : re.sub(r\"[^\\w\\s.]\", \" \", text)).to_list()\n",
    "        data = self.frame['Question'].to_list()\n",
    "        for text in data:\n",
    "            yield [chars for chars in jieba.lcut(str(text).strip())]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.inputs[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetF(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len) -> None:\n",
    "        super(NetF, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.embedding_layer = torch.nn.Embedding(vocab_size, 64)\n",
    "        self.fc_layer = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(seq_len * 64, 128),\n",
    "            torch.nn.Tanh())\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        outputs = self.embedding_layer(inputs)\n",
    "        outputs = self.fc_layer(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetG(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len) -> None:\n",
    "        super(NetG, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.embedding_layer = torch.nn.Embedding(vocab_size, 128)\n",
    "        self.rnn_layer = torch.nn.GRU(128, 256, batch_first=True)\n",
    "        self.fc_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(256, vocab_size),\n",
    "            torch.nn.Softmax(1))\n",
    "    \n",
    "    def init_hiddens(self, batch_size, use_cuda = True):\n",
    "        if use_cuda:\n",
    "            return torch.zeros(1, batch_size, 256).cuda()\n",
    "        return torch.zeros(1, batch_size, 256).cpu()\n",
    "\n",
    "    def forward(self, inputs, hiddens):\n",
    "        embedded = self.embedding_layer(inputs)\n",
    "        outputs, hiddens = self.rnn_layer(embedded, hiddens)\n",
    "        outputs = torch.squeeze(outputs, dim=1)\n",
    "        outputs = self.fc_layer(outputs)\n",
    "        return outputs, hiddens\n",
    "    \n",
    "    def sample(self, n = 1, use_cuda = False):\n",
    "        seq_outputs = torch.zeros(n, self.seq_len).long()\n",
    "        z_inputs = torch.zeros(n, 1).long()\n",
    "        hiddens = self.init_hiddens(n, use_cuda)\n",
    "\n",
    "        if use_cuda:\n",
    "            seq_outputs = seq_outputs.cuda()\n",
    "            z_inputs = z_inputs.cuda()\n",
    "            \n",
    "        for i in range(self.seq_len):\n",
    "            outputs, hiddens = self(z_inputs, hiddens)\n",
    "            outputs = torch.distributions.Categorical(probs=outputs)\n",
    "            outputs = outputs.sample()\n",
    "            seq_outputs[:, i] = outputs\n",
    "            z_inputs = torch.unsqueeze(outputs, dim=1)\n",
    "            \n",
    "        return seq_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetR(torch.nn.Module):\n",
    "    def __init__(self, vocab_size) -> None:\n",
    "        super(NetR, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_layer = torch.nn.Embedding(vocab_size, 128)\n",
    "        self.rnn_layer = torch.nn.GRU(128, 256, batch_first=True)\n",
    "        self.fc_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(256, vocab_size),\n",
    "            torch.nn.Softmax(2))\n",
    "        \n",
    "    def init_hiddens(self, batch_size, use_cuda = True):\n",
    "        if use_cuda:\n",
    "            return torch.zeros(1, batch_size, 256).cuda()\n",
    "        return torch.zeros(1, batch_size, 256).cpu()\n",
    "    \n",
    "    def forward(self, inputs, hiddens):\n",
    "        embedded = self.embedding_layer(inputs)\n",
    "        outputs, _ = self.rnn_layer(embedded, hiddens)\n",
    "        outputs = self.fc_layer(outputs)\n",
    "        outputs = torch.distributions.Categorical(probs=outputs)\n",
    "        outputs = outputs.sample()\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ktpss\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.616 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "694\n"
     ]
    }
   ],
   "source": [
    "gamma = 1\n",
    "seq_len = 40\n",
    "batch_size = 64\n",
    "sample_size = 100\n",
    "lr = 0.001\n",
    "epochs = 20\n",
    "\n",
    "dataset = TextDataset(seq_len=seq_len)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "vocab_size = len(dataset.stoi)\n",
    "print(vocab_size)\n",
    "\n",
    "F = NetF(vocab_size, seq_len)\n",
    "G = NetG(vocab_size, seq_len)\n",
    "R = NetR(vocab_size)\n",
    "\n",
    "F = F.cuda()\n",
    "G = G.cuda()\n",
    "R = R.cuda()\n",
    "\n",
    "g_optim = torch.optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999)) \n",
    "r_optim = torch.optim.Adam(R.parameters(), lr=lr, betas=(0.5, 0.999)) \n",
    "cosine =  torch.nn.CosineSimilarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Epoch: [1/20] G_Loss: 4.1706 R_Loss: 0.0001\n",
      "[+] Epoch: [1/20] G_Loss: 0.8703 R_Loss: 0.0005\n",
      "[+] Epoch: [1/20] G_Loss: 0.6919 R_Loss: -0.0008\n",
      "[+] Epoch: [1/20] G_Loss: 0.6109 R_Loss: 0.0016\n",
      "[+] Epoch: [2/20] G_Loss: 0.6714 R_Loss: 0.0001\n",
      "[+] Epoch: [2/20] G_Loss: 0.5695 R_Loss: 0.0005\n",
      "[+] Epoch: [2/20] G_Loss: 0.5071 R_Loss: 0.0013\n",
      "[+] Epoch: [2/20] G_Loss: 0.4581 R_Loss: 0.0008\n",
      "[+] Epoch: [3/20] G_Loss: 0.4216 R_Loss: 0.0005\n",
      "[+] Epoch: [3/20] G_Loss: 0.4317 R_Loss: -0.0010\n",
      "[+] Epoch: [3/20] G_Loss: 0.3946 R_Loss: 0.0007\n",
      "[+] Epoch: [3/20] G_Loss: 0.3770 R_Loss: 0.0011\n",
      "[+] Epoch: [4/20] G_Loss: 0.3172 R_Loss: 0.0002\n",
      "[+] Epoch: [4/20] G_Loss: 0.3047 R_Loss: 0.0004\n",
      "[+] Epoch: [4/20] G_Loss: 0.3168 R_Loss: -0.0008\n",
      "[+] Epoch: [4/20] G_Loss: 0.3333 R_Loss: -0.0004\n",
      "[+] Epoch: [5/20] G_Loss: 0.2908 R_Loss: -0.0016\n",
      "[+] Epoch: [5/20] G_Loss: 0.2689 R_Loss: -0.0013\n",
      "[+] Epoch: [5/20] G_Loss: 0.2865 R_Loss: -0.0011\n",
      "[+] Epoch: [5/20] G_Loss: 0.2924 R_Loss: 0.0020\n",
      "[+] Epoch: [6/20] G_Loss: 0.2452 R_Loss: 0.0003\n",
      "[+] Epoch: [6/20] G_Loss: 0.2320 R_Loss: -0.0013\n",
      "[+] Epoch: [6/20] G_Loss: 0.2653 R_Loss: -0.0009\n",
      "[+] Epoch: [6/20] G_Loss: 0.2108 R_Loss: -0.0011\n",
      "[+] Epoch: [7/20] G_Loss: 0.2015 R_Loss: 0.0011\n",
      "[+] Epoch: [7/20] G_Loss: 0.2241 R_Loss: -0.0008\n",
      "[+] Epoch: [7/20] G_Loss: 0.2031 R_Loss: 0.0016\n",
      "[+] Epoch: [7/20] G_Loss: 0.2025 R_Loss: -0.0017\n",
      "[+] Epoch: [8/20] G_Loss: 0.2379 R_Loss: 0.0007\n",
      "[+] Epoch: [8/20] G_Loss: 0.2032 R_Loss: 0.0013\n",
      "[+] Epoch: [8/20] G_Loss: 0.2077 R_Loss: 0.0012\n",
      "[+] Epoch: [8/20] G_Loss: 0.1949 R_Loss: -0.0009\n",
      "[+] Epoch: [9/20] G_Loss: 0.1788 R_Loss: 0.0001\n",
      "[+] Epoch: [9/20] G_Loss: 0.1853 R_Loss: -0.0006\n",
      "[+] Epoch: [9/20] G_Loss: 0.2141 R_Loss: -0.0001\n",
      "[+] Epoch: [9/20] G_Loss: 0.1769 R_Loss: -0.0011\n",
      "[+] Epoch: [10/20] G_Loss: 0.1888 R_Loss: 0.0003\n",
      "[+] Epoch: [10/20] G_Loss: 0.2028 R_Loss: -0.0004\n",
      "[+] Epoch: [10/20] G_Loss: 0.1828 R_Loss: 0.0009\n",
      "[+] Epoch: [10/20] G_Loss: 0.1997 R_Loss: -0.0022\n",
      "[+] Epoch: [11/20] G_Loss: 0.1859 R_Loss: 0.0014\n",
      "[+] Epoch: [11/20] G_Loss: 0.1815 R_Loss: -0.0002\n",
      "[+] Epoch: [11/20] G_Loss: 0.1787 R_Loss: 0.0006\n",
      "[+] Epoch: [11/20] G_Loss: 0.1863 R_Loss: 0.0002\n",
      "[+] Epoch: [12/20] G_Loss: 0.1567 R_Loss: 0.0012\n",
      "[+] Epoch: [12/20] G_Loss: 0.1881 R_Loss: -0.0001\n",
      "[+] Epoch: [12/20] G_Loss: 0.1647 R_Loss: -0.0000\n",
      "[+] Epoch: [12/20] G_Loss: 0.1595 R_Loss: 0.0009\n",
      "[+] Epoch: [13/20] G_Loss: 0.1481 R_Loss: 0.0007\n",
      "[+] Epoch: [13/20] G_Loss: 0.1567 R_Loss: -0.0023\n",
      "[+] Epoch: [13/20] G_Loss: 0.1533 R_Loss: 0.0004\n",
      "[+] Epoch: [13/20] G_Loss: 0.1653 R_Loss: -0.0005\n",
      "[+] Epoch: [14/20] G_Loss: 0.1580 R_Loss: 0.0004\n",
      "[+] Epoch: [14/20] G_Loss: 0.1702 R_Loss: 0.0010\n",
      "[+] Epoch: [14/20] G_Loss: 0.1726 R_Loss: 0.0006\n",
      "[+] Epoch: [14/20] G_Loss: 0.1664 R_Loss: -0.0022\n",
      "[+] Epoch: [15/20] G_Loss: 0.1692 R_Loss: 0.0002\n",
      "[+] Epoch: [15/20] G_Loss: 0.1570 R_Loss: 0.0008\n",
      "[+] Epoch: [15/20] G_Loss: 0.1462 R_Loss: -0.0006\n",
      "[+] Epoch: [15/20] G_Loss: 0.1539 R_Loss: -0.0008\n",
      "[+] Epoch: [16/20] G_Loss: 0.1476 R_Loss: -0.0012\n",
      "[+] Epoch: [16/20] G_Loss: 0.1480 R_Loss: -0.0005\n",
      "[+] Epoch: [16/20] G_Loss: 0.1497 R_Loss: 0.0003\n",
      "[+] Epoch: [16/20] G_Loss: 0.1477 R_Loss: -0.0013\n",
      "[+] Epoch: [17/20] G_Loss: 0.1373 R_Loss: 0.0007\n",
      "[+] Epoch: [17/20] G_Loss: 0.1388 R_Loss: 0.0003\n",
      "[+] Epoch: [17/20] G_Loss: 0.1482 R_Loss: -0.0012\n",
      "[+] Epoch: [17/20] G_Loss: 0.1465 R_Loss: -0.0005\n",
      "[+] Epoch: [18/20] G_Loss: 0.1436 R_Loss: -0.0008\n",
      "[+] Epoch: [18/20] G_Loss: 0.1411 R_Loss: 0.0019\n",
      "[+] Epoch: [18/20] G_Loss: 0.1557 R_Loss: -0.0001\n",
      "[+] Epoch: [18/20] G_Loss: 0.1443 R_Loss: 0.0013\n",
      "[+] Epoch: [19/20] G_Loss: 0.1515 R_Loss: 0.0001\n",
      "[+] Epoch: [19/20] G_Loss: 0.1350 R_Loss: -0.0003\n",
      "[+] Epoch: [19/20] G_Loss: 0.1374 R_Loss: 0.0015\n",
      "[+] Epoch: [19/20] G_Loss: 0.1406 R_Loss: -0.0019\n",
      "[+] Epoch: [20/20] G_Loss: 0.1268 R_Loss: 0.0001\n",
      "[+] Epoch: [20/20] G_Loss: 0.1366 R_Loss: 0.0015\n",
      "[+] Epoch: [20/20] G_Loss: 0.1322 R_Loss: 0.0012\n",
      "[+] Epoch: [20/20] G_Loss: 0.1463 R_Loss: 0.0010\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for iters, inputs in enumerate(dataloader, 0):\n",
    "        sample_index = random.randint(0, batch_size - 1)\n",
    "        real_inputs = inputs.cuda()\n",
    "        generate_inputs = G.sample(batch_size, True)\n",
    "\n",
    "        hiddens = R.init_hiddens(batch_size)\n",
    "\n",
    "        g_sample = generate_inputs\n",
    "        g_sample[sample_index:sample_index+1, :] = real_inputs[sample_index:sample_index+1, :]\n",
    "        ranked_sample = R(g_sample, hiddens)\n",
    "\n",
    "        ys = F(generate_inputs)\n",
    "        yu = F(ranked_sample)\n",
    "        alpha = cosine(ys, yu)\n",
    "        rewards = torch.exp(gamma * alpha) / torch.sum(torch.exp(gamma * alpha), dim=0)\n",
    "\n",
    "        g_loss = 0.0\n",
    "        hiddens = G.init_hiddens(batch_size)\n",
    "        for i in range(seq_len - 1):\n",
    "            z_inputs = real_inputs[:, i:i+1]\n",
    "            outputs, hiddens = G(z_inputs, hiddens)\n",
    "            for j in range(batch_size):\n",
    "                g_loss += -torch.log(outputs[j, real_inputs[j, i+1]]) * rewards[j]\n",
    "        \n",
    "        g_optim.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optim.step()\n",
    "\n",
    "        g_loss /= batch_size\n",
    "\n",
    "        sample_index = random.randint(0, batch_size - 1)\n",
    "        with torch.no_grad():\n",
    "            g_sample = G.sample(1, True)\n",
    "        \n",
    "        hiddens = R.init_hiddens(batch_size)\n",
    "\n",
    "        rc_sample = real_inputs\n",
    "        fc_sample = generate_inputs\n",
    "\n",
    "        rc_sample[sample_index:sample_index+1, :] = g_sample\n",
    "        fc_sample[sample_index:sample_index+1, :] = real_inputs[sample_index:sample_index+1, :]\n",
    "\n",
    "        real_ranked = R(rc_sample, hiddens)\n",
    "        fake_ranked = R(fc_sample, hiddens)\n",
    "\n",
    "        real_ys = F(real_inputs)\n",
    "        real_yu = F(real_ranked)\n",
    "        real_alpha = cosine(real_ys, real_yu)\n",
    "        real_p = torch.exp(gamma * real_alpha) / torch.sum(torch.exp(gamma * real_alpha), dim=0)\n",
    "\n",
    "        fake_ys = F(generate_inputs)\n",
    "        fake_yu = F(fake_ranked)\n",
    "        fake_alpha = cosine(fake_ys, fake_yu)\n",
    "        fake_p = torch.exp(gamma * fake_alpha) / torch.sum(torch.exp(gamma * fake_alpha), dim=0)\n",
    "\n",
    "        r_loss = torch.mean(torch.log(real_p)) - torch.mean(torch.log(fake_p))\n",
    "                \n",
    "        r_optim.zero_grad()\n",
    "        r_loss.backward()\n",
    "        r_optim.step()\n",
    "\n",
    "        if iters % 10 == 0:\n",
    "            print(\"[+] Epoch: [%d/%d] G_Loss: %.4f R_Loss: %.4f\" % (epoch+1, epochs, g_loss, r_loss))\n",
    "            with torch.no_grad():\n",
    "                fd = open(f\"epoch_{epoch+1}_step_{iters}.txt\", \"w\", encoding='utf-8')\n",
    "                generate_data = G.cpu().sample(sample_size)\n",
    "                for i in range(sample_size):\n",
    "                    text = \"\"\n",
    "                    for j in range(seq_len):\n",
    "                        text += dataset.itos[generate_data[i][j].item()]\n",
    "                    fd.write(text + \"\\n\")\n",
    "                fd.close()\n",
    "            G.cuda()\n",
    "\n",
    "G = G.cpu().eval()\n",
    "R = R.cpu().eval()\n",
    "\n",
    "torch.save(G, \"rankgan_modelG.pth\")\n",
    "torch.save(R, \"rankgan_modelR.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = open(f\"sample.txt\", \"w\", encoding='utf-8')\n",
    "itos = pickle.load(open(\"./itos.bin\", \"rb\"))\n",
    "model = torch.load(\"./rankgan_modelG.pth\")\n",
    "sample_size = 200\n",
    "seq_len = 40\n",
    "generate_data = model.sample(sample_size)\n",
    "for i in range(sample_size):\n",
    "    text = \"\"\n",
    "    for j in range(seq_len):\n",
    "        if itos[generate_data[i][j].item()] == \"'\":\n",
    "            break\n",
    "        text += itos[generate_data[i][j].item()]\n",
    "    fd.write(text + \"\\n\")\n",
    "fd.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
