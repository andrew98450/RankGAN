{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import pickle\n",
    "import jieba\n",
    "import random\n",
    "#import re\n",
    "import pandas\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root=\"./CDC.csv\", seq_len = 40) -> None:\n",
    "        super(TextDataset, self).__init__()\n",
    "        self.frame = pandas.read_csv(root)\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab = build_vocab_from_iterator(self.get_vocab(), specials=[\"<start>\", \"'\"])\n",
    "        self.stoi = self.vocab.get_stoi()\n",
    "        self.itos = self.vocab.get_itos()\n",
    "        self.inputs = self.get_inputs()\n",
    "        pickle.dump(self.stoi, open(\"stoi.bin\", \"wb\"))\n",
    "        pickle.dump(self.itos, open(\"itos.bin\", \"wb\"))\n",
    "\n",
    "    def get_inputs(self):\n",
    "        #data = self.frame['Question'].apply(lambda text : re.sub(r\"[^\\w\\s.]\", \" \", text)).to_list()\n",
    "        data = self.frame['Question'].to_list()\n",
    "        inputs = []\n",
    "        for text in data:\n",
    "            text_data = [self.stoi[chars] for chars in jieba.lcut(str(text).strip())]\n",
    "            text_data.insert(0, self.stoi[\"<start>\"])\n",
    "            while len(text_data) < self.seq_len:\n",
    "                text_data.append(self.stoi[\"'\"])\n",
    "            inputs.append(text_data[:self.seq_len])\n",
    "        inputs = numpy.array(inputs, dtype=numpy.int32)\n",
    "        inputs = torch.from_numpy(inputs).long()\n",
    "        return inputs\n",
    "    \n",
    "    def get_vocab(self):\n",
    "        #data = self.frame['Question'].apply(lambda text : re.sub(r\"[^\\w\\s.]\", \" \", text)).to_list()\n",
    "        data = self.frame['Question'].to_list()\n",
    "        for text in data:\n",
    "            yield [chars for chars in jieba.lcut(str(text).strip())]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.inputs[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetF(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len) -> None:\n",
    "        super(NetF, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.embedding_layer = torch.nn.Embedding(vocab_size, 64)\n",
    "        self.fc_layer = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(seq_len * 64, 128),\n",
    "            torch.nn.Tanh())\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        outputs = self.embedding_layer(inputs)\n",
    "        outputs = self.fc_layer(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetG(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len) -> None:\n",
    "        super(NetG, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.embedding_layer = torch.nn.Embedding(vocab_size, 128)\n",
    "        self.rnn_layer = torch.nn.GRU(128, 256, batch_first=True)\n",
    "        self.fc_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(256, vocab_size),\n",
    "            torch.nn.Softmax(1))\n",
    "    \n",
    "    def init_hiddens(self, batch_size, use_cuda = True):\n",
    "        if use_cuda:\n",
    "            return torch.zeros(1, batch_size, 256).cuda()\n",
    "        return torch.zeros(1, batch_size, 256).cpu()\n",
    "\n",
    "    def forward(self, inputs, hiddens):\n",
    "        embedded = self.embedding_layer(inputs)\n",
    "        outputs, hiddens = self.rnn_layer(embedded, hiddens)\n",
    "        outputs = torch.squeeze(outputs, dim=1)\n",
    "        outputs = self.fc_layer(outputs)\n",
    "        return outputs, hiddens\n",
    "    \n",
    "    def sample(self, n = 1, use_cuda = False):\n",
    "        seq_outputs = torch.zeros(n, self.seq_len).long()\n",
    "        z_inputs = torch.zeros(n, 1).long()\n",
    "        hiddens = self.init_hiddens(n, use_cuda)\n",
    "\n",
    "        if use_cuda:\n",
    "            seq_outputs = seq_outputs.cuda()\n",
    "            z_inputs = z_inputs.cuda()\n",
    "            \n",
    "        for i in range(self.seq_len):\n",
    "            outputs, hiddens = self(z_inputs, hiddens)\n",
    "            outputs = torch.distributions.Categorical(probs=outputs)\n",
    "            outputs = outputs.sample()\n",
    "            seq_outputs[:, i] = outputs\n",
    "            z_inputs = torch.unsqueeze(outputs, dim=1)\n",
    "            \n",
    "        return seq_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetR(torch.nn.Module):\n",
    "    def __init__(self, vocab_size) -> None:\n",
    "        super(NetR, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_layer = torch.nn.Embedding(vocab_size, 128)\n",
    "        self.rnn_layer = torch.nn.GRU(128, 256, batch_first=True)\n",
    "        self.fc_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(256, vocab_size),\n",
    "            torch.nn.Softmax(2))\n",
    "        \n",
    "    def init_hiddens(self, batch_size, use_cuda = True):\n",
    "        if use_cuda:\n",
    "            return torch.zeros(1, batch_size, 256).cuda()\n",
    "        return torch.zeros(1, batch_size, 256).cpu()\n",
    "    \n",
    "    def forward(self, inputs, hiddens):\n",
    "        embedded = self.embedding_layer(inputs)\n",
    "        outputs, _ = self.rnn_layer(embedded, hiddens)\n",
    "        outputs = self.fc_layer(outputs)\n",
    "        outputs = torch.distributions.Categorical(probs=outputs)\n",
    "        outputs = outputs.sample()\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1\n",
    "seq_len = 40\n",
    "batch_size = 64\n",
    "sample_size = 100\n",
    "lr = 0.001\n",
    "epochs = 20\n",
    "\n",
    "dataset = TextDataset(seq_len=seq_len)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "vocab_size = len(dataset.stoi)\n",
    "print(vocab_size)\n",
    "\n",
    "F = NetF(vocab_size, seq_len)\n",
    "G = NetG(vocab_size, seq_len)\n",
    "R = NetR(vocab_size)\n",
    "\n",
    "F = F.cuda()\n",
    "G = G.cuda()\n",
    "R = R.cuda()\n",
    "\n",
    "g_optim = torch.optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999)) \n",
    "r_optim = torch.optim.Adam(R.parameters(), lr=lr, betas=(0.5, 0.999)) \n",
    "cosine =  torch.nn.CosineSimilarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for iters, inputs in enumerate(dataloader, 0):\n",
    "        sample_index = random.randint(0, batch_size - 1)\n",
    "        real_inputs = inputs.cuda()\n",
    "        generate_inputs = G.sample(batch_size, True)\n",
    "\n",
    "        hiddens = R.init_hiddens(batch_size)\n",
    "\n",
    "        g_sample = generate_inputs\n",
    "        g_sample[sample_index:sample_index+1, :] = real_inputs[sample_index:sample_index+1, :]\n",
    "        ranked_sample = R(g_sample, hiddens)\n",
    "\n",
    "        ys = F(generate_inputs)\n",
    "        yu = F(ranked_sample)\n",
    "        alpha = cosine(ys, yu)\n",
    "        rewards = torch.exp(gamma * alpha) / torch.sum(torch.exp(gamma * alpha), dim=0)\n",
    "\n",
    "        g_loss = 0.0\n",
    "        hiddens = G.init_hiddens(batch_size)\n",
    "        for i in range(seq_len - 1):\n",
    "            z_inputs = real_inputs[:, i:i+1]\n",
    "            outputs, hiddens = G(z_inputs, hiddens)\n",
    "            for j in range(batch_size):\n",
    "                g_loss += -torch.log(outputs[j, real_inputs[j, i+1]]) * rewards[j]\n",
    "        \n",
    "        g_optim.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optim.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            g_sample = G.sample(1, True)\n",
    "            g_sample = torch.distributions.Categorical(probs=g_sample)\n",
    "            g_sample = g_sample.sample()\n",
    "\n",
    "        rc_sample = real_inputs\n",
    "        fc_sample = generate_inputs\n",
    "\n",
    "        rc_sample[sample_index:sample_index+1, :] = g_sample\n",
    "        fc_sample[sample_index:sample_index+1, :] = real_inputs[sample_index:sample_index+1, :]\n",
    "\n",
    "        real_ranked = R(rc_sample, hiddens)\n",
    "        fake_ranked = R(fc_sample, hiddens)\n",
    "\n",
    "        real_ys = F(real_inputs)\n",
    "        real_yu = F(real_ranked)\n",
    "        real_alpha = cosine(real_ys, real_yu)\n",
    "        real_p = torch.exp(gamma * real_alpha) / torch.sum(torch.exp(gamma * real_alpha), dim=0)\n",
    "\n",
    "        fake_ys = F(generate_inputs)\n",
    "        fake_yu = F(fake_ranked)\n",
    "        fake_alpha = cosine(fake_ys, fake_yu)\n",
    "        fake_p = torch.exp(gamma * fake_alpha) / torch.sum(torch.exp(gamma * fake_alpha), dim=0)\n",
    "\n",
    "        r_loss = torch.mean(torch.log(real_p)) - torch.mean(torch.log(fake_p))\n",
    "                \n",
    "        r_optim.zero_grad()\n",
    "        r_loss.backward()\n",
    "        r_optim.step()\n",
    "\n",
    "        if iters % 10 == 0:\n",
    "            print(\"[+] Epoch: [%d/%d] G_Loss: %.4f R_Loss: %.4f\" % (epoch+1, epochs, g_loss, r_loss))\n",
    "            with torch.no_grad():\n",
    "                fd = open(f\"epoch_{epoch+1}_step_{iters}.txt\", \"w\", encoding='utf-8')\n",
    "                generate_data = G.cpu().sample(sample_size)\n",
    "                for i in range(sample_size):\n",
    "                    text = \"\"\n",
    "                    for j in range(seq_len):\n",
    "                        text += dataset.itos[generate_data[i][j].item()]\n",
    "                    fd.write(text + \"\\n\")\n",
    "                fd.close()\n",
    "            G.cuda()\n",
    "\n",
    "G = G.cpu().eval()\n",
    "R = R.cpu().eval()\n",
    "\n",
    "torch.save(G, \"rankgan_modelG.pth\")\n",
    "torch.save(R, \"rankgan_modelR.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = open(f\"sample.txt\", \"w\", encoding='utf-8')\n",
    "itos = pickle.load(open(\"./itos.bin\", \"rb\"))\n",
    "model = torch.load(\"./rankgan_modelG.pth\")\n",
    "sample_size = 50\n",
    "seq_len = 40\n",
    "generate_data = model.sample(sample_size)\n",
    "for i in range(sample_size):\n",
    "    text = \"\"\n",
    "    for j in range(seq_len):\n",
    "        if itos[generate_data[i][j].item()] == \"'\":\n",
    "            break\n",
    "        text += itos[generate_data[i][j].item()]\n",
    "    fd.write(text + \"\\n\")\n",
    "fd.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
